User: I'm working on curi

User: I'm working on @curiculum/02*.md The reasarch is a bit vauge, Can you more clearly epxlain what I need to look into

Assistant: You’re working on `curriculum/02-phase-1-system-framing-and-contracts.md`.
The “Research” section is asking for a decision-ready spec, not generic reading.

Use this as your exact target for `docs/phase1/research.md`:

1. Ad-serving lifecycle (end-to-end)
- Show the step sequence: client request -> decisioning -> response -> impression/click events -> logging/storage.
- For each step, define input, output, failure modes, and retry behavior.
- Include one “happy path” and one “failure path” flow.

2. HTTP semantics
- Justify method choice for each endpoint (`/decision`, `/impression`, `/click`).
- Define status code matrix for success + client errors + server errors.
- Define error response shape consistency across endpoints.

3. Idempotency for event endpoints
- State duplicate-event risk and where it comes from (retries, network issues, client re-send).
- Define idempotency key strategy (what key, TTL/window, storage/check logic).
- Define behavior for duplicate events (same response? ignored? logged?).

4. SLI/SLOs
- Pick measurable SLIs for this phase, e.g. decision latency, 5xx rate, invalid request rate, event ingestion success.
- Set concrete SLO targets (with window), e.g. p95 latency under X ms over 7 days.
- Explain how each SLI is measured and where data comes from.

5. At least 12 system invariants (with why)
- Write each invariant as a strict “must always” rule.
- Add one-line rationale: what breaks if violated.
- Cover input validation, ordering, dedupe, logging correctness, and error handling.

A good invariant looks like:
- “An impression must reference an existing decision ID; otherwise reject with 4xx.”
- “The same idempotency key for `/click` must not create two stored click events.”
- “All rejected requests must return structured error schema with machine-readable code.”

If you want, I can draft a ready-to-fill `docs/phase1/research.md` template with all sections and example invariant slots.

User: I'm still not clear on this. How should I describe it? Should I go something like

1. Do you want me to write a diagram? Write it in a table? How should I explain this?

Assistant: Yes. Use both: one simple diagram + tables.
That’s the clearest and easiest to grade.

Use this structure in `docs/phase1/research.md`:

1. `## 1) Request Lifecycle`
- Add a Mermaid sequence diagram (happy path).
- Add a 1-table failure path.

2. `## 2) HTTP Semantics`
- Table: `Endpoint | Method | Why this method | 2xx | 4xx cases | 5xx cases`.

3. `## 3) Idempotency`
- Table: `Endpoint | Idempotency key | Dedup window | Duplicate behavior | Storage/check strategy`.

4. `## 4) SLI/SLO`
- Table: `SLI | Definition | SLO target | Measurement window | Data source`.

5. `## 5) System Invariants (>=12)`
- Table: `Invariant ID | Must-always rule | Why it matters | How enforced/tested`.

Use short explanations (1-3 sentences per section), and put most detail in tables.

If you want, I can generate a full copy-paste markdown template for this file right now.

User: How can I do mermaid diagrams in markdown

Assistant: Use a fenced code block with `mermaid`.

```markdown
```mermaid
sequenceDiagram
    participant C as Client
    participant A as API
    participant D as Decision Service
    participant L as Logger

    C->>A: POST /decision
    A->>D: Validate + compute decision
    D-->>A: decision_id + ad response
    A-->>C: 200 OK + decision payload

    C->>A: POST /impression (decision_id, idempotency_key)
    A->>L: Record impression
    A-->>C: 202 Accepted
```
```

Tips:
1. GitHub, GitLab, and many docs tools render Mermaid automatically.
2. If your renderer doesn’t support Mermaid, keep a fallback text flow below it.
3. Keep it simple: 5-8 steps is enough for this phase.

If you want, I can add a ready-to-use Mermaid diagram directly into `docs/phase1/research.md`.

User: No I'll do it myself. I'm writing it in vscode and when I check the preview it isnt rendering. Is there are an extension I could use that would render the diagrams?

Assistant: Yes, use this VS Code extension:

1. Install `Markdown Preview Mermaid Support` (`bierner.markdown-mermaid`).
2. Re-open preview with “Markdown: Open Preview to the Side”.
3. Make sure your block is exactly:

```markdown
```mermaid
flowchart TD
  A[Start] --> B[End]
```
```

If it still doesn’t render:
1. Update VS Code to latest.
2. In the preview pane, click the security/shield icon and allow preview content for the workspace.
3. Confirm the language tag is `mermaid` (not `mmd` or `markdown`).

User: Use this structure in docs/phase1/research.md:
  1. ## 1) Request Lifecycle

  - Add a Mermaid sequence diagram (happy path).
  - Add a 1-table failure path.

What do you mean by 1-table failure path?

Assistant: “1-table failure path” means: after your happy-path Mermaid diagram, add a single markdown table that explains what happens when things go wrong.

Use one row per failure scenario, for example:

| Step | Failure scenario | Detection | API response | Retry? | Logged fields |
|---|---|---|---|---|---|
| `/decision` validation | Missing required field | Schema validator fails | `400` with error code | No | request_id, error_code |
| Decisioning | Timeout to rules/data service | Upstream timeout | `503` | Yes (client/backoff) | request_id, timeout_ms |
| `/impression` | Duplicate idempotency key | Key already seen | `200/202` duplicate-ack | No new write | decision_id, idempotency_key |
| `/click` | Unknown `decision_id` | Lookup miss | `404` or `422` | No | decision_id, error_code |

So “1-table” = one consolidated failure table, not multiple separate failure sections.

User: [Image attached showing text: "- Table: Endpoint | Method | Why this method | 2xx | 4xx cases | 5xx cases."]
Is the table different from this table that you also added?

Assistant: Yes, it’s different.

The one in your screenshot is the HTTP Semantics table:
- `Endpoint | Method | Why this method | 2xx | 4xx cases | 5xx cases`

The “1-table failure path” I mentioned is for the Request Lifecycle section and focuses on flow breakdowns:
- `Step | Failure scenario | Detection | API response | Retry? | Logged fields`

You can keep both.
If you want to reduce duplication, reference the HTTP table from the failure table instead of repeating every status code detail.

User: Alright I'm working on the failure scenarious currently. Any tips on what I should search to figure out more failing scenarious and what I should be guarding against?

Assistant: Great place to focus. Use targeted searches and a simple classification method.

Search queries to paste:
1. `ad serving api failure modes decision impression click`
2. `idempotency key design for event ingestion`
3. `distributed systems duplicate event handling at-least-once delivery`
4. `http status code best practices validation vs conflict vs unprocessable`
5. `api timeout retry backoff circuit breaker patterns`
6. `event logging failure scenarios partial failure consistency`
7. `SLO error budget common API SLIs`

Failure categories to cover (at least 1-2 each):
1. Input issues: missing fields, wrong types, invalid enum, oversized payload.
2. Contract issues: unknown `decision_id`, stale version, unsupported schema.
3. Duplicate/concurrency: replayed `/impression` or `/click`, race conditions.
4. Dependency failures: DB down, cache miss/stale, upstream timeout.
5. Network failures: client timeout, retry storms, partial writes.
6. Security/abuse: malformed spam, rate-limit breaches, forged IDs.
7. Observability failures: request succeeds but logging/metrics fail.

For each scenario, document this exact shape:
1. Trigger.
2. Detection signal.
3. API response (`4xx/5xx` + error code).
4. Retry policy (yes/no, backoff).
5. Guardrail (validation, idempotency store, timeout, circuit breaker, DLQ, etc).
6. Test you’ll add.

Guardrails you almost always want:
1. Strict schema validation + clear error model.
2. Idempotency key + dedupe window for event endpoints.
3. Timeouts on all downstream calls.
4. Retry with backoff only for transient failures.
5. Rate limiting + request size limits.
6. Structured logging with `request_id`, `decision_id`, `idempotency_key`.

If you want, I can give you 12 concrete failure scenarios pre-filled in table format for `phase1`.

User: I've move on to researching system invarients and I just want to make sure I'm understanding what is required of me. I need to provide 12 invariances of my system and explain why they must be so. An example one is #request > # impressions. There can not possibly be more impressions then request since an impression can not happen without a request

Assistant: Yes, you’re understanding it correctly.

What they want is:
1. `>=12` must-always-be-true rules about your system.
2. For each rule, explain why violation is dangerous.
3. Ideally also state how it’s enforced/tested (this helps grading).

Your example is good in spirit, but make it more precise and model-safe:

- Instead of `#requests > #impressions`, write:
  `Every accepted impression must reference an existing decision_id.`
  Why: prevents orphan impressions and bad billing/analytics.

If your model is strictly one impression per decision, then this is also valid:
- `accepted_impressions <= accepted_decisions`
  Why: cannot record more unique impressions than decisions.

Quick template per invariant:
- `Invariant`: …
- `Why it matters`: …
- `Enforced by`: validation/idempotency/DB constraint
- `Test`: test name or scenario

If you want, I can help you turn your current draft into 12 strong, testable invariants.

User: What do you mean by make it model-safe?

Assistant: “Model-safe” means phrasing invariants so they stay true under your actual business model and data semantics, not just intuition.

Your original:
- `#requests > #impressions`

Why risky:
- Not all requests become impressions.
- Retries/duplicates can inflate raw counts.
- Some models allow multiple impressions per request/decision.

Safer version:
- `Every accepted impression must reference an existing decision_id.`
- If one impression per decision: `Unique accepted impressions <= accepted decisions.`

So model-safe = precise, testable, and valid even with retries, dedupe rules, and your endpoint contracts.

User: Hmm ok. so it souends like I need to have a business model in mind first before I decide on systme invariants

Assistant: Exactly. You need a minimal system model first.

You don’t need a full business plan, just these decisions:
1. Entity relationships: request -> decision -> impression/click.
2. Cardinality: one or many impressions/clicks per decision.
3. Acceptance rules: what makes events valid/invalid.
4. Dedup policy: idempotency key behavior and window.
5. Time/order assumptions: max lateness, expiry, out-of-order handling.

Then write invariants against that model.
If you want, I can give you a small default model for this phase and 12 invariants aligned to it.

User: For the time/order assumptions:
- max lateness: I'm not really sure what this mean
- expiry: Also not sue what we are expiring
- out-of-order handling. What do we do if we recieve a click event before an impression event I'm guessing?

Assistant: `max lateness`, `expiry`, and `out-of-order` are just rules for event timing.

Use this simple Phase 1 policy:

1. `Max lateness`
- Meaning: oldest allowed event age.
- Example rule: reject events where `now - event_timestamp > 24h`.
- Why: prevents very old/replayed events from corrupting metrics.

2. `Expiry`
- Usually you expire:
- `decision_id` validity window (example: valid for 24h).
- `idempotency_key` dedupe record (example: keep for 24h or 7d).
- Why: bound storage and define when events are no longer acceptable.

3. `Out-of-order handling`
- Yes, like click before impression.
- Pick one policy and enforce it everywhere:
- `Simpler`: accept click if `decision_id` is valid, even if impression not yet seen; log anomaly.
- `Stricter`: require impression first; otherwise reject or hold briefly then re-check.

For this curriculum, I’d use the simpler policy (less implementation risk), then encode it as invariants + tests.

User: Can you write our entire conversation up to this point to a file called context.txt
